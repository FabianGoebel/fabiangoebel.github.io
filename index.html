<!DOCTYPE HTML>
<!--
        Read Only by HTML5 UP
        html5up.net | @n33co
        Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <title>Fabian Goebel</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
  </head>
  <body class="is-preload">
  <!-- Header -->
  <section id="header">
    <header>
      <span class="image avatar">
        <img src="images/avatar.jpg" alt="" />
      </span>
      <h1 id="logo">
        <a href="#">Fabian Goebel</a>
      </h1>
      <p>Doctoral Student at ETH Zürich
      <br/>Institute of Cartography and Geoinformation</p>
    </header>
    <nav id="nav">
      <ul>
        <li>
          <a href="#one" class="active">About</a>
        </li>
        <li>
          <a href="#proj">Research</a>
        </li>
        <li>
          <a href="#pupl">Publications</a>
        </li>
        <li>
          <a href="#mail">Contact</a>
        </li>
      </ul>
    </nav>
    <footer>
      <ul class="icons">
        <li>
		  <a href="#mail" class="icon solid fa-envelope">
            <span class="label">Email</span>
          </a>
		  </li>
		  <li>
          <a href="https://www.researchgate.net/profile/Fabian_Goebel" target="_blank" class="icon solid fa-book">
            <span class="label">ResearchGate</span>
          </a>
        </li>
        <li>
          <a href="https://www.linkedin.com/in/fgoebel" target="_blank" class="icon brands fa-linkedin" size="20">
            <span class="label">LinkedIn</span>
          </a>
        </li>
        <li>
          <a href="https://www.xing.com/profile/Fabian_Goebel7" target="_blank" class="icon brands fa-xing">
            <span class="label">Xing</span>
          </a>
        </li>
      </ul>
    </footer>
  </section>
  <!-- Wrapper -->
  <div id="wrapper">
    <!-- Main -->
    <div id="main">
    <!-- One -->
    <section id="one">
      <div class="container">
        <header class="major">
          <h2>Fabian Göbel</h2>
          <p>Dipl. Media Computer Scientist</p>
        </header>
        <p>Since February 2016 I am a doctorate student at the
        <a href="http://www.geogaze.org" target="_blank">GeoGazeLab</a> at the ETH Zürich, Swizerland. My research is focused on
        Eye Tracking and Implicit Interface Adaptions. In 2015 I received my Diploma Degree in Media Computer Science at the
        <a href="https://imld.de" target="_blank">Interactive Media Lab</a> at the Dresden University of Technology.</p>
      </div>
    </section>
    <!-- Two -->
    <section id="proj">
	<div class="container">
    <h3>Research Projects</h3>	
	<span class="image left">
        <a href="http://www.geogaze.org/igamaps/">
          <img src="images/IGAMAPS.gif" alt="IGAMaps" />
        </a>
    </span>
    <h4>Intention-Aware Gaze-Based Assistance on Maps (IGAMaps)</h4>Adaptive human computer interfaces can significantly 
	facilitate the interaction with digital maps in application scenarios such as navigation, information search, and for 
	users with physical limitations. A map system that is able to recognize its user’s information requirements fast and accurately 
	could adapt the map content, thus increasing effectiveness and efficiency of interaction. Since maps are primarily perceived and 
	explored with the visual sense, it is plausible to assume that a user’s gaze on a map can be used to recognize intentions and activities, 
	which can then trigger map adaptation.The main goal of this research project consists in the investigation of methods for the recognition 
	of activities and intentions from visual attention during the interaction with maps. The project tries to answer the following questions:
	<ul>
		<li>Can we infer a map user’s activity and intention from the visual attention he or she is spending on the map (e.g., route planning, searching for a restaurant)?</li>
		<li>How should a map adaptation be designed to be helpful?</li>
		<li>What is the general user acceptance of gaze-based intention recognition on maps?</li>
	</ul>
	
    <br />
    <br />
    <p align="right"><a class="button primary small" href="http://www.geogaze.org/igamaps/" target="_blank">
      More
    </a>
    <hr>
	
      <span class="image left">
        <a href="https://imld.de/en/study/theses/2015-2/diplomarbeit-fabian-goebel/">
          <img src="images/DA_Fabian-Goebel.jpg" alt="" />
        </a>
      </span>
    <h4>Diploma Thesis &quot;A Concept to Leverage the Visual Periphery for Parallel Information&quot;</h4>Over the last years,
    display screens have drastically improved not only with respect to resolution but also in terms of dimension. Many applications
    utilize this space for presenting more information as well as to support multiple task by still applying conventional UI
    paradigms. Considering peoples visual focus of attention to be limited by human factors, intelligent UIs have to be developed
    to take better advantage of the gained space. This work investigates, using the visual periphery for displaying information
    while focusing on a primary task. Based on the understanding of the human visual system, vital aspects for perceiving
    information within the field of view have been tested in a lab study. The outcome indicates that secondary information can be
    quickly and reliably perceived in the periphery without significantly affecting a primary task. Carefully considering the study
    results and the participants’ feedback, a concept for a novel visual attentive UI is elaborated. It combines eye tracking
    with intelligent UI adaptions to make more information perceivable without affecting the primary task performance and thus, use
    huge displays more efficiently.
    <br />
    <br />
    <p align="right"><a class="button primary small" href="https://imld.de/en/study/theses/theses-2015/diplomarbeit-fabian-goebel/" target="_blank">
      More
    </a>
    <hr>

      <span class="image left">
        <a href="https://imld.de/en/research/research-projects/gaze-supported-foot-interaction/">
          <img src="images/GBFI.jpg" alt="" />
        </a>
      </span>

    <h4>Gaze-supported Foot Interaction</h4>When working with zoomable information spaces, we can distinguish complex tasks into
    primary and secondary tasks (e.g., pan and zoom). In this context, a multimodal combination of gaze and foot input is highly
    promising for supporting manual interactions, for example, using mouse and keyboard. Motivated by this, we present several
    alternatives for multimodal gaze-supported foot interaction in a computer desktop setup for pan and zoom. While our eye gaze is
    ideal to indicate a user’s current point of interest and where to zoom in, foot interaction is well suited for parallel input
    controls, for example, to specify the zooming speed. Our investigation focuses on varied foot input devices differing in their
    degree of freedom (e.g., one- and two-directional foot pedals) that can be seamlessly combined with gaze input.
    <br />
    <br />
    <p align="right"><a class="button primary small" href="https://imld.de/en/research/research-projects/gaze-supported-foot-interaction/" target="_blank">
      More
    </a>
	<hr>

      <span class="image left">
        <a href="https://vimeo.com/37264194">
          <img src="images/depthtouch.jpg" alt="" />
        </a>
      </span>

    <h4>DepthTouch: an elastic surface for tangible computing</h4>DepthTouch is an installation which explores future interactive surfaces and features
    elastic feedback, allowing the user to go deeper than with regular multi-touch surfaces. DepthTouch’s elastic display allows
    the user to create valleys and ascending slopes by depressing or grabbing its textile surface. We describe the experimental
    approach for eliciting appropriate interaction metaphors from interaction with real materials and the resulting digital
    prototype.
    <br />
    <br />
    <p align="right"><a class="button primary small" href="https://vimeo.com/37264194" target="_blank">More
    </a>
	</div>
	</section>
    <section id="pupl">
      <div class="container">
        <h3>Publications</h3>
        <div class="table-wrapper">
          <table>
            <tbody>
				<tr>
			    <td>2020</td>
				<td><b>Göbel F.</b>, Kurzhals K., Schinazi V. R., Kiefer P., Raubal M. (2020).</br>
				<i>Gaze-Adaptive Lenses for Feature-Rich Information Spaces.</i> In Proceedings of ETRA '20 Proceedings of the 12th ACM Symposium on Eye Tracking Research & Applications (ETRA &#39;20), Stuttgart, Germany, 2020.
                <br/>
				<a href="https://doi.org/10.1145/3379155.3391323" target="_blank">DOI: 10.1145/3379155.3391323</a>
                </td>
              </tr>
				<tr>
			    <td>2020</td>
				<td><b>Göbel F.</b>, Kurzhals K., Raubal M., Schinazi V. R. (2020).</br>
				<i>Gaze-Aware Mixed-Reality: Addressing Privacy Issues with Eye Tracking. </i> In CHI 2020 Workshop on &quot;Exploring Potentially Abusive Ethical, Social and Political Implications of Mixed Reality in HCI&quot;. (CHI &#39;20), Honolulu, Hawaii, USA, 2020.
                <br/>
                </td>
              </tr>
			<tr>
			    <td>2020</td>
				<td>Kurzhals K., <b>Göbel F.</b>, Angerbauer K., Sedlmair M., Raubal M. (2020). (Honorable mention)</br>
				<i>A View on the Viewer: Gaze-Adaptive Captions for Videos. </i> In Proceedings of CHI Conference on Human Factors in Computing Systems (CHI &#39;20), Honolulu, Hawaii, USA, 2020.
                <br/>
				<a href="https://doi.org/10.1145/3313831.3376266" target="_blank">DOI: 10.1145/3313831.3376266</a>
                </td>
              </tr>
			<tr>
			    <td>2019</td>
				<td><b>Göbel F.</b>, Kiefer P. (2019).</br>
				<i>POITrack: Improving Map-Based Planning with Implicit POI Tracking. </i> In Eye Tracking for Spatial Research (ET4S@ETRA &#39;19), Denver, Colorado, USA, 2019.
                <br/>
				<a href="https://doi.org/10.1145/3317959.3321491" target="_blank">DOI: 10.1145/3317959.3321491</a>
                </td>
              </tr>
			  <tr>
			    <td>2019</td>
				<td>Wissen Hayek, U., Müller, K., <b>Göbel F.</b>, Kiefer, P., Spielhofer, R. and Grêt-Regamey, A.(2019).</br>
				<i>3D Point Clouds and Eye Tracking for Investigating the Perception and Acceptance of Power Lines in Different Landscapes.</i> Multimodal Technologies Interact.
                <br/>
				<a href="https://doi.org/10.3390/mti3020040" target="_blank">DOI: 10.3390/mti3020040</a>
                </td>
              </tr>
			<tr>
			    <td>2019</td>
				<td><b>Göbel F.</b>, Kwok C.K. T. and Rudi D. (2019).</br>
				<i>Look There! Be Social and Share. </i>In CHI 2019 Workshop on &quot;Challenges Using Head-Mounted Displays in Shared and Social Spaces&quot;. CHI &#39;19, Glasgow, Scotland UK, 2019.
                <br/>
				<a href="https://doi.org/10.3929/ethz-b-000331280" target="_blank">DOI: 10.3929/ethz-b-000331280</a>
                </td>
              </tr>
			<tr>
			    <td>2019</td>
				<td><b>Göbel F.</b>, Kiefer P. and Raubal M. (2019).</br>
				<i>FeaturEyeTrack: automatic matching of eye tracking data with map features on interactive maps. </i>Geoinformatica (2019).
                <br/>
                <a href="https://doi.org/10.1007/s10707-019-00344-3" target="_blank">DOI: 10.1007/s10707-019-00344-3</a></td>
              </tr>
			<tr>
			    <td>2018</td>
				<td><b>Göbel F.</b>, Kiefer P., Giannopoulos I. and Raubal M. (2018).</br>
				<i>Gaze Sequences and Map Task Complexity. </i> In Proceedings of the 10th International Conference on Geographic Information  Science (GIScience 2018) 2018, Melbourne, Australia.
                <br/>
                <a href="https://doi.org/10.4230/LIPIcs.GISCIENCE.2018.30" target="_blank">DOI: 10.4230/LIPIcs.GISCIENCE.2018.30</a></td>
              </tr>
			  <tr>
				<td>2018</td>
				<td><b>Göbel F.</b> and Martin H. (2018).</br>
				<i> Unsupervised Clustering of Eye Tracking Data. </i>Spatial Big Data and Machine Learning in GIScience, Workshop at GIScience 2018, Melbourne, Australia, 2018.
                <br/>
				<a href="https://doi.org/10.3929/ethz-b-000290476" target="_blank">DOI: 10.3929/ethz-b-000290476</a></td>
              </tr>
			  <tr>
				<td>2018</td>
				<td><b>Göbel F.</b>, Kiefer P., Giannopoulos I., Duchowski, A.T. and Raubal M. (2018).</br>
				<i> Improving Map Reading with Gaze-Adaptive Legends. </i> In ETRA ’18: 2018 Symposium on Eye Tracking Research and Applications.
                <br/>
                <a href="https://doi.org/10.1145/3204493.3204544" target="_blank">DOI: 10.1145/3204493.3204544</a></td>
              </tr>
                <td>2018</td>
                <td>Kiefer P., Giannopoulos I., <b>Göbel F.</b>, Raubal M. and Duchowski, A.T. (2018).</br><i> 
				ET4S Eye Tracking for Spatial Research, Proceedings of the 3rd International Workshop.</i> 
                <br />
                <a href="https://doi.org/10.3929/ethz-b-000222256" target="_blank">DOI: 10.3929/ethz-b-000222256</a></td>
				</tr>
				<tr>
				<td>2018</td>
				<td><b>Göbel F.</b>, Bakogiannis N., Henggeler K., Tschümperlin R., Xu Y., Kiefer P. and Raubal M. (2018).</br>
				<i> A Public Gaze-Controlled Campus Map. </i> 3rd International Workshop on Eye Tracking for Spatial Research.
                <br/>
                <a href="https://doi.org/10.3929/ethz-b-000222491" target="_blank">DOI: 10.3929/ethz-b-000222491</a></td>
              </tr>
			<tr>
                <td>2017</td>
                <td><b>Göbel F.</b>, Kiefer P. and Raubal M. (2017).</br><i> 
				FeaturEyeTrack: A Vector Tile-Based Eye Tracking Framework for Interactive Maps.</i> 
				In Societal Geo-Innovation : short papers, posters and poster abstracts of the 20th AGILE Conference on Geographic Information Science, Editors: A Bregt, T Sarjakoski, R. van Lammeren, F. Rip
                <br/>
                <a href="https://n.ethz.ch/~goebelf/download/103_ShortPaper_in_PDF.pdf" target="_blank">Download</a></td>
              </tr>
			   <tr>
                <td>2016</td>
                <td><b>Göbel F.</b>, Giannopoulos I. and Raubal M. (2016).</br>
				<i> The Importance of Visual Attention for Adaptive Interfaces.</i> 
				In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct (MobileHCI '16), Florence, Italy
                <br/>
                <a href="http://dx.doi.org/10.1145/2957265.2962659" target="_blank">DOI: 10.1145/2957265.2962659</a></td>
              </tr>
              <tr>
                <td>2015</td>
                <td>Klamka K, Siegel A., Vogt S.,
                <b>Göbel F.</b>, Stellmach S. and Dachselt R. (2015).</br> <i> Look &amp; Pedal: Hands-free Navigation in Zoomable Information
                Spaces through Gaze-supported Foot Input.</i>  In Proceedings of the 2015 ACM on International Conference on Multimodal
                Interaction (ICMI &#39;15). ACM, New York, NY, USA, 123-130.
                <br/>
                <a href="http://dx.doi.org/10.1145/2818346.2820751" target="_blank">DOI: 10.1145/2818346.2820751</a></td>
              </tr>
              <tr>
                <td>2013</td>
                <td>
                <b>Göbel F.</b>, Klamka K, Siegel A., Vogt S., Stellmach S. and Dachselt R. (2013).</br> <i> Gaze-supported Foot Interaction in
                Zoomable Information Spaces. Interactivity at CHI &#39;13.</i>  In Proc. of CHI &#39;13 Extended Abstracts on Human
                Factors in Computing Systems (CHI EA &#39;13). ACM, New York, NY, USA, 3059-3062.
                <br/>
                <a href="http://dx.doi.org/10.1145/2468356.2479610" target="_blank">DOI: 10.1145/2468356.2479610</a></td>
              </tr>
              <tr>
                <td>2013</td>
                <td>
                <b>Göbel F.</b>, Klamka K, Siegel A., Vogt S., Stellmach S. and Dachselt R. (2013).</br> <i> Gaze-supported Foot Interaction
                in Zoomable Information Spaces.</i>  In CHI 2013 Workshop on &quot;Gaze Interaction in the Post-WIMP
                World&quot;.CHI &#39;13, Paris, France, April 27, 2013.
                <br/>
                <a href="https://n.ethz.ch/~goebelf/download/2013-CHI-GazeSupportedFootInteraction_Workshop.pdf" target="_blank">Download</a></td>
              </tr>
              <tr>
                <td>2012</td>
                <td>Peschke J.,
                <b>Göbel F.</b> and Groh R. (2012).</br><i>  DepthTouch: Elastische Membran zwischen virtuellem und realem Raum.</i>  In:
                Reiterer, H. &amp; Deussen, O. (Hrsg.), Mensch &amp; Computer 2012 – Workshopband: interaktiv informiert –
                allgegenwärtig und allumfassend!? München: Oldenbourg Verlag. (S. 493-496)
                <br/>
                <a href="http://dl.mensch-und-computer.de/handle/123456789/2935" target="_blank">ISBN: 978-3-486-71990-1</a></td>
              </tr>
              <tr>
                <td>2012</td>
                <td>Peschke J.,
                <b>Göbel F.</b>, Gründer T., Keck M., Kammer D. and Groh R. (2012).</br> <i> DepthTouch: an elastic surface for tangible
                computing. </i> In Proceedings of the International Working Conference on Advanced Visual Interfaces (AVI &#39;12),
                Genny Tortora, Stefano Levialdi, and Maurizio Tucci (Eds.). ACM, New York, NY, USA, 770-771.
                <br/>
                <a href="http://dx.doi.org/10.1145/2254556.2254706" target="_blank">DOI: 10.1145/2254556.2254706</a></td>
              </tr>
            </tbody>
          </table>

        <h3>Academic Honors and Achievements</h3>
        <div class="table-wrapper">
          <table>
            <tbody>
			  <tr>
                <td>2020</td>
                <td>CHI 2020, Honorable mention for “A View on the Viewer: Gaze-Adaptive Captions for Videos.”</td>
              </tr>
              <tr>
                <td>2014</td>
                <td>Visiting student (1 week) at Microsoft Research Cambridge, Great Britain</td>
              </tr>
              <tr>
                <td>2012</td>
                <td>Mensch und Computer 2012, Best Demo award for Project “DepthTouch”</td>
              </tr>
            </tbody>
          </table>
        </div>

        <h3>Other Professional Activities</h3>
        <div class="table-wrapper">
          <table>
            <tbody>
              <tr>
                <td>2013</td>
                <td>Demonstration of “Gaze-supported Foot Interaction in Zoomable Information Spaces“ as Interactivity at
                <a href="http://chi2013.acm.org" target="_blank"></a>“ACM Conference on Human Factors in Computing Systems“,
                2013, Paris, France</td>
              </tr>
              <tr>
                <td>2013</td>
                <td>Presentation of “Gaze-supported Foot Interaction in Zoomable Information Spaces“ at the CHI &#39;13
                Workshop on
                <a href="http://gaze-interaction.net/chi13-workshop" target="_blank">&quot;Gaze Interaction in the Post-WIMP
                World&quot;</a>. Paris, France</td>
              </tr>
              <tr>
                <td>2013</td>
                <td>Demonstration of “Gaze-supported Foot Interaction in Zoomable Information Spaces“ at
                <a href="http://output-dd.de/project/blickgestuetzte-fussinteraktion" target="_blank">&quot;OUTPUT.DD&quot;</a>,
                Dresden, Germany</td>
              </tr>
              <tr>
                <td>2012</td>
                <td>Demonstration “DepthTouch“ at
                <a href="http://output-dd.de/content/depthtouch-begreifbare-interaktion-mit-der-virtualit%C3%83%C2%A4t"
                target="_blank">&quot;OUTPUT.DD&quot;</a>, Dresden, Germany</td>
              </tr>
              <tr>
                <td>2012</td>
                <td>Demonstration of “DepthTouch“ at
                <a href="http://www.mb21.de" target="_blank">“MB21-Festival 2012“</a>, a festival on multimedia for children,
                “Medienkulturzentrum Dresden e.V“., Dresden, Germany</td>
              </tr>
            </tbody>
          </table>
        </div>
    </section>
    <!-- Four -->
    <section id="mail">
      <div class="container">
        <h3>Contact Me</h3>
		<h4>Fabian Göbel</h4>
		PhD Student, ETH Zurich</br>
		Geoinformation Engineering</br></br>
		<a href="http://www.ikg.ethz.ch/" target="_blank">Institute of Cartography and Geoinformation</a></br>
		<a href="http://www.gis.ethz.ch/en/" target="_blank">Chair of Geoinformation Engineering</a></br></br>
		
		<a href="http://www.mapsearch.ethz.ch/map.do?farbcode=c000&gebaeudeMap=HIL&" target="_blank">HIL</a><a href="http://www.rauminfo.ethz.ch/Rauminfo/grundrissplan.gif?gebaeude=HIL&geschoss=G&raumNr=28.1&" target="_blank">G 23.1</a></br>
		Stefano-Franscini-Platz 5</br>
		8093 Zurich, Switzerland</br>
</p>
        <ul class="icons">
          <li>
            <a href="mailto:goebelf@ethz.ch" class="icon solid fa-envelope">
              <span class="label">Mail</span>
            </a>
          </li>
          <li>
            <a href="https://www.researchgate.net/profile/Fabian_Goebel" target="_blank" class="icon solid fa-book">
              <span class="label">ResearchGate</span>
            </a>
          </li>
          <li>
            <a href="https://www.linkedin.com/in/fgoebel" target="_blank" class="icon brands fa-linkedin">
              <span class="label">LinkedIn</span>
            </a>
          </li>
          <li>
            <a href="https://www.xing.com/profile/Fabian_Goebel7" target="_blank" class="icon brands fa-xing">
              <span class="label">Xing</span>
            </a>
          </li>
        </ul>
      </div>
    </section>
    <!-- Footer -->
    <section id="footer">
      <div class="container">
        <ul class="copyright">
          <li>Fabian Göbel. All rights reserved.</li>
          <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
        </ul>
      </div>
    </section>
  </div>
  
  <!-- Scripts -->
  <script src="assets/js/jquery.min.js"></script>
  <script src="assets/js/jquery.scrollex.min.js"></script>
  <script src="assets/js/jquery.scrolly.min.js"></script>
  <script src="assets/js/browser.min.js"></script>
  <script src="assets/js/breakpoints.min.js"></script>
  <script src="assets/js/util.js"></script>
  <script src="assets/js/main.js"></script>
</body>
</html>
